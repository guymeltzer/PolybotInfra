# ENHANCED OUTPUTS - Comprehensive cluster information
# 
# Provides organized, informative outputs with actual values where possible
# Includes worker node information, ArgoCD details, and comprehensive cluster status

# ------------------------------------------------------------------------
# Section 1: ğŸ—ï¸ INFRASTRUCTURE OVERVIEW
# ------------------------------------------------------------------------

output "cluster_overview" {
  description = "ğŸ—ï¸ Complete cluster infrastructure overview"
  value = {
    "ğŸŒ Region" = var.region
    "ğŸ·ï¸ Cluster Name" = local.cluster_name
    "ğŸŒ VPC ID" = module.k8s-cluster.vpc_id
    "ğŸŒ Domain" = var.domain_name
    "ğŸ“ Kubeconfig Location" = local.kubeconfig_path
  }
}

output "control_plane_info" {
  description = "ğŸ–¥ï¸ Control plane detailed information"
  value = {
    "ğŸ“ Instance ID" = module.k8s-cluster.control_plane_instance_id
    "ğŸŒ Public IP" = module.k8s-cluster.control_plane_public_ip
    "ğŸ”’ Private IP" = module.k8s-cluster.control_plane_private_ip
    "ğŸ”— API Endpoint" = "https://${module.k8s-cluster.control_plane_public_ip}:6443"
    "ğŸ”‘ SSH Key" = module.k8s-cluster.ssh_key_name
    "ğŸ’» SSH Command" = "ssh -i ${module.k8s-cluster.ssh_key_name}.pem ubuntu@${module.k8s-cluster.control_plane_public_ip}"
  }
}

output "worker_nodes_info" {
  description = "ğŸ¤– Worker nodes and Auto Scaling information"
  value = {
    "ğŸ¤– ASG Name" = module.k8s-cluster.worker_asg_name
    "ğŸš€ Launch Template" = module.k8s-cluster.worker_launch_template_id
    "ğŸ“Š Desired Workers" = var.desired_worker_nodes
    "ğŸ” Discovery Command" = "aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names ${module.k8s-cluster.worker_asg_name} --region ${var.region} --query 'AutoScalingGroups[0].{Desired:DesiredCapacity,Min:MinSize,Max:MaxSize,Current:length(Instances)}' --output table"
    "ğŸ“‹ List Workers" = "aws ec2 describe-instances --region ${var.region} --filters 'Name=tag:aws:autoscaling:groupName,Values=${module.k8s-cluster.worker_asg_name}' 'Name=instance-state-name,Values=running' --query 'Reservations[*].Instances[*].{Name:Tags[?Key==`Name`]|[0].Value,InstanceId:InstanceId,PrivateIP:PrivateIpAddress,PublicIP:PublicIpAddress,State:State.Name}' --output table"
  }
}

output "network_resources" {
  description = "ğŸŒ Network and load balancing information"
  value = {
    "âš–ï¸ ALB DNS Name" = module.k8s-cluster.alb_dns_name
    "ğŸŒ ALB Zone ID" = module.k8s-cluster.alb_zone_id
    "ğŸ”— Application URL" = "https://${var.domain_name}"
    "ğŸ  Public Subnets" = module.k8s-cluster.public_subnet_ids
    "ğŸ”’ Private Subnets" = module.k8s-cluster.private_subnet_ids
  }
}

# ------------------------------------------------------------------------
# Section 2: â˜¸ï¸ KUBERNETES CLUSTER STATUS
# ------------------------------------------------------------------------

output "kubernetes_access" {
  description = "â˜¸ï¸ Kubernetes cluster access information"
  value = {
    "ğŸ“ Kubeconfig Path" = local.kubeconfig_path
    "âœ… Kubeconfig Exists" = fileexists(local.kubeconfig_path)
    "ğŸ” Kubeconfig Secret" = module.k8s-cluster.kubeconfig_secret_name_output
    "ğŸ« Join Command Secret" = module.k8s-cluster.kubernetes_join_command_secrets.latest_secret
    "ğŸ’» Quick Setup" = "export KUBECONFIG=${local.kubeconfig_path} && kubectl get nodes"
  }
}

output "cluster_commands" {
  description = "ğŸ› ï¸ Essential cluster management commands"
  value = {
    "ğŸ“Š Check Nodes" = "kubectl --kubeconfig=${local.kubeconfig_path} get nodes -o wide"
    "ğŸ” Check Pods" = "kubectl --kubeconfig=${local.kubeconfig_path} get pods --all-namespaces"
    "ğŸ¥ Cluster Health" = "kubectl --kubeconfig=${local.kubeconfig_path} get componentstatuses"
    "ğŸ“‹ Cluster Info" = "kubectl --kubeconfig=${local.kubeconfig_path} cluster-info"
  }
}

# ------------------------------------------------------------------------
# Section 3: ğŸš€ ARGOCD GITOPS PLATFORM
# ------------------------------------------------------------------------

output "argocd_complete_access" {
  description = "ğŸš€ Complete ArgoCD access and management information"
  value = {
    "ğŸŒ Access URL" = "https://localhost:8080"
    "ğŸ‘¤ Username" = "admin"
    "ğŸ”‘ Password Retrieval" = "kubectl --kubeconfig=${local.kubeconfig_path} -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' 2>/dev/null | base64 -d || echo 'Secret not found or changed'"
    "ğŸ”— Port Forward Command" = "kubectl --kubeconfig=${local.kubeconfig_path} port-forward svc/argocd-server -n argocd 8080:443"
    "ğŸ“± Applications Check" = "kubectl --kubeconfig=${local.kubeconfig_path} get applications -n argocd"
    "ğŸ” Controller Logs" = "kubectl --kubeconfig=${local.kubeconfig_path} logs -n argocd -l app.kubernetes.io/name=argocd-application-controller -f"
    "ğŸ¯ Expected Apps" = ["mongodb", "polybot", "yolo5"]
  }
}

# ------------------------------------------------------------------------
# Section 4: ğŸ”§ AWS RESOURCES & INFRASTRUCTURE
# ------------------------------------------------------------------------

output "aws_infrastructure" {
  description = "ğŸ”§ AWS resources and infrastructure details"
  value = {
    "ğŸ—ï¸ Core Infrastructure" = {
      "Region" = var.region
      "VPC ID" = module.k8s-cluster.vpc_id
      "Public Subnets" = length(module.k8s-cluster.public_subnet_ids)
      "Private Subnets" = length(module.k8s-cluster.private_subnet_ids)
    }
    "ğŸ–¥ï¸ Compute Resources" = {
      "Control Plane Instance" = module.k8s-cluster.control_plane_instance_id
      "Worker ASG" = module.k8s-cluster.worker_asg_name
      "Launch Template" = module.k8s-cluster.worker_launch_template_id
      "Desired Workers" = var.desired_worker_nodes
    }
    "ğŸ” Security & Access" = {
      "SSH Key Name" = module.k8s-cluster.ssh_key_name
      "Control Plane IAM Role" = module.k8s-cluster.control_plane_iam_role_arn
    }
    "ğŸ’¾ Storage & Logs" = {
      "Worker Logs Bucket" = module.k8s-cluster.worker_logs_bucket
    }
  }
}

# ------------------------------------------------------------------------
# Section 5: ğŸ› ï¸ QUICK START & TROUBLESHOOTING
# ------------------------------------------------------------------------

output "quick_start_guide" {
  description = "ğŸ› ï¸ Complete quick start guide"
  value = <<-EOT
ğŸ‰ POLYBOT KUBERNETES CLUSTER READY! ğŸ‰

ğŸ“‹ QUICK START STEPS:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ 1ï¸âƒ£ SET UP KUBECTL ACCESS:                                       â”‚
â”‚    export KUBECONFIG=${local.kubeconfig_path}                   â”‚
â”‚    kubectl get nodes                                            â”‚
â”‚                                                                â”‚
â”‚ 2ï¸âƒ£ ACCESS ARGOCD UI:                                            â”‚
â”‚    kubectl port-forward svc/argocd-server -n argocd 8080:443   â”‚
â”‚    Open: https://localhost:8080                                â”‚
â”‚    Username: admin                                             â”‚
â”‚    Password: kubectl get secret argocd-initial-admin-secret    â”‚
â”‚             -n argocd -o jsonpath='{.data.password}' | base64 -dâ”‚
â”‚                                                                â”‚
â”‚ 3ï¸âƒ£ SSH TO CONTROL PLANE:                                        â”‚
â”‚    ssh -i ${module.k8s-cluster.ssh_key_name}.pem ubuntu@${module.k8s-cluster.control_plane_public_ip}                â”‚
â”‚                                                                â”‚
â”‚ 4ï¸âƒ£ CHECK CLUSTER STATUS:                                        â”‚
â”‚    kubectl get pods --all-namespaces                          â”‚
â”‚    kubectl get applications -n argocd                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

ğŸ” TROUBLESHOOTING:
â€¢ Logs: kubectl logs -n argocd -l app.kubernetes.io/name=argocd-application-controller
â€¢ Workers: aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names ${module.k8s-cluster.worker_asg_name}
â€¢ Health: kubectl get componentstatuses
EOT
}

# ------------------------------------------------------------------------
# Section 6: ğŸ¯ NEXT STEPS
# ------------------------------------------------------------------------

output "next_steps" {
  description = "ğŸ¯ Recommended next steps"
  value = {
    "immediate_actions" = [
      "ğŸ”— Access ArgoCD UI using the port-forward command above",
      "ğŸ” Verify applications are syncing properly in ArgoCD",
      "ğŸš€ Deploy additional applications via ArgoCD or kubectl",
      "ğŸ“Š Monitor cluster health and application status"
    ]
    "monitoring_setup" = [
      "ğŸ”§ Configure monitoring and logging as needed",
      "ğŸ“ˆ Set up alerts for cluster health",
      "ğŸ’¾ Configure backup strategies"
    ]
    "security_hardening" = [
      "ğŸ”’ Review and harden security settings",
      "ğŸ›¡ï¸ Implement network policies",
      "ğŸ” Rotate default passwords and secrets"
    ]
  }
}

# Legacy outputs (kept for compatibility)
output "control_plane_ip" {
  description = "Public IP address of the Kubernetes control plane"
  value       = module.k8s-cluster.control_plane_public_ip
}

output "control_plane_id" {
  description = "Instance ID of the Kubernetes control plane"
  value       = module.k8s-cluster.control_plane_instance_id
}

output "worker_asg_name" {
  description = "Name of the Auto Scaling Group for worker nodes"
  value       = module.k8s-cluster.worker_asg_name
}

output "load_balancer_dns" {
  description = "DNS name of the load balancer"
  value       = module.k8s-cluster.alb_dns_name
}

output "vpc_id" {
  description = "VPC ID where cluster is deployed"
  value       = module.k8s-cluster.vpc_id
}

# Remove or simplify these old outputs
output "cluster_info" {
  description = "Basic cluster information"
  value = {
    region              = var.region
    control_plane_ip    = module.k8s-cluster.control_plane_public_ip
    control_plane_id    = module.k8s-cluster.control_plane_instance_id
    api_endpoint        = "https://${module.k8s-cluster.control_plane_public_ip}:6443"
    kubeconfig_path     = local.kubeconfig_path
  }
}

output "cluster_readiness" {
  description = "Information about cluster readiness"
  value = {
    kubeconfig_exists = fileexists(local.kubeconfig_path)
  }
}

output "argocd_access" {
  description = "ArgoCD access information"
  value = {
    url                = "https://localhost:8080"
    username          = "admin"
    password_command  = "kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d"
    port_forward_help = "kubectl port-forward svc/argocd-server -n argocd 8080:443"
  }
}

output "aws_resources" {
  description = "AWS resources created"
  sensitive   = true
  value = {
    vpc_id         = module.k8s-cluster.vpc_id
    public_subnets = module.k8s-cluster.public_subnet_ids
    ssh_key        = module.k8s-cluster.ssh_key_name
    alb_dns        = module.k8s-cluster.alb_dns_name
  }
}

output "ssh_commands" {
  description = "SSH commands for cluster access"
  sensitive   = true
  value = {
    control_plane = "ssh -i ${module.k8s-cluster.ssh_key_name}.pem ubuntu@${module.k8s-cluster.control_plane_public_ip}"
    worker_template = "ssh -i ${module.k8s-cluster.ssh_key_name}.pem ubuntu@WORKER_IP"
  }
}

output "kubectl_setup" {
  description = "Commands to set up kubectl access"
  sensitive   = true
  value = {
    copy_kubeconfig = "ssh -i ${module.k8s-cluster.ssh_key_name}.pem ubuntu@${module.k8s-cluster.control_plane_public_ip} 'cat ~/.kube/config' > kubeconfig.yaml"
    set_kubeconfig  = "export KUBECONFIG=./kubeconfig.yaml"
    test_cluster    = "kubectl get nodes"
  }
}

output "troubleshooting" {
  description = "Commands for troubleshooting"
  sensitive   = true
  value = {
    check_control_plane = "ssh -i ${module.k8s-cluster.ssh_key_name}.pem ubuntu@${module.k8s-cluster.control_plane_public_ip} 'kubectl get nodes'"
    check_worker_logs   = "aws s3 ls s3://guy-polybot-logs/ --recursive | grep worker-init"
    check_asg_instances = "aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names ${module.k8s-cluster.worker_asg_name} --region ${var.region}"
    view_init_logs      = "ssh -i ${module.k8s-cluster.ssh_key_name}.pem ubuntu@${module.k8s-cluster.control_plane_public_ip} 'sudo cat /var/log/k8s-init.log'"
  }
}

output "quick_start" {
  description = "Quick start commands"
  sensitive   = true
  value = <<-EOT
    1. Copy kubeconfig:
       ssh -i ${module.k8s-cluster.ssh_key_name}.pem ubuntu@${module.k8s-cluster.control_plane_public_ip} 'cat ~/.kube/config' > kubeconfig.yaml
    
    2. Set kubeconfig:
       export KUBECONFIG=./kubeconfig.yaml
    
    3. Verify cluster:
       kubectl get nodes
    
    4. Access ArgoCD:
       kubectl port-forward svc/argocd-server -n argocd 8080:443
       # Then visit https://localhost:8080
       # Username: admin
       # Password: kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d
  EOT
}

output "generated_secrets_info" {
  description = "Information about generated secrets and credentials"
  sensitive   = true
  value = {
    ssh_key_generated = var.key_name == "" ? true : false
    ssh_key_location = var.key_name == "" ? "${path.module}/polybot-key.pem" : "Using provided key: ${var.key_name}"
    argocd_password_location = "kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d"
  }
}